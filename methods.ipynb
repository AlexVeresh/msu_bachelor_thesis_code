{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brat_parser import get_entities_relations_attributes_groups\n",
    "import random\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer, AutoModel, PreTrainedTokenizer, AutoModelForSequenceClassification\n",
    "from bertviz import head_view, model_view\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers.trainer_callback import dataclass\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "from bertviz import model_view, head_view\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995185b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4600a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text, items_to_replace):\n",
    "    result = text\n",
    "    for item in items_to_replace:\n",
    "        result = result.replace(item, '')\n",
    "    return result    \n",
    "\n",
    "def format_smta_data(data_str):\n",
    "    data = json.loads(data_str)\n",
    "    result = []\n",
    "    print(data['text'])\n",
    "    text = nltk.word_tokenize(clear_sentence(data['text']))\n",
    "    for item in data['index']:\n",
    "        if 'SMTA' in item['type']:\n",
    "            print(text[item['pos'] - 1 : item['pos'] + item['len'] - 1])\n",
    "            new_item = item.copy()\n",
    "            new_item.pop('wt')\n",
    "            new_item.pop('sent')\n",
    "            result.append(new_item)\n",
    "    return result\n",
    "\n",
    "def read_dicts_data(dictname, size):\n",
    "    result = []\n",
    "    for i in range(size):\n",
    "        file = open(dictname + '/output_' + str(i) +'.txt', 'r')\n",
    "        result.append(format_smta_data(file.read()))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_tokens_and_attention(sentence):\n",
    "    inputs = tokenizer.encode_plus(sentence, return_tensors='pt', add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention = model(input_ids)[-1]\n",
    "    input_id_list = input_ids[0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "    return attention, tokens\n",
    "\n",
    "def map_type_to_tag(sentiment_type):\n",
    "    if \"NEG\" in sentiment_type:\n",
    "        return \"1\";\n",
    "    else:\n",
    "        return \"-1\";\n",
    "\n",
    "\n",
    "def ids_of_dashes(data):\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        if data[i] == '-':\n",
    "            start = i-1\n",
    "            if i + 2 >= len(data):\n",
    "                return [start, i+2]\n",
    "            else:    \n",
    "                i += 2\n",
    "                while i < len(data) and data[i] == '-':\n",
    "                    i += 2\n",
    "                return [start, i]\n",
    "        i += 1\n",
    "    \n",
    "    return None\n",
    "            \n",
    "\n",
    "def sublist(sl, l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return [ind,ind+sll]\n",
    "\n",
    "        \n",
    "def all_sentiments_as_map(entities, relations):\n",
    "    res = dict()\n",
    "    for key, r in relations.items():\n",
    "        if r.type == \"OPINION_RELATES_TO\":\n",
    "            key_obj = entities[r.obj].text\n",
    "            if key_obj in res:\n",
    "                res[key_obj].append((map_type_to_tag(entities[r.subj].text), entities[r.subj].text))\n",
    "            else:\n",
    "                res[key_obj] = [(map_type_to_tag(entities[r.subj].text), entities[r.subj].text)]\n",
    "        elif r.type == \"POS_AUTHOR_FROM\" or r.type == \"NEG_AUTHOR_FROM\":\n",
    "            key_subj = entities[r.subj].text\n",
    "            if key_subj in res:\n",
    "                res[key_subj].append((map_type_to_tag(entities[r.obj].text), entities[r.obj].text))\n",
    "            else:\n",
    "                res[key_subj] = [(map_type_to_tag(entities[r.obj].text), entities[r.obj].text)]\n",
    "    return res\n",
    "\n",
    "def all_sentiments_as_list(entities, relations):\n",
    "    res = []\n",
    "    for key, r in relations.items():\n",
    "        if r.type == \"OPINION_RELATES_TO\":\n",
    "            res.append((map_type_to_tag(entities[r.subj].text), entities[r.subj].text))\n",
    "        elif r.type == \"POS_AUTHOR_FROM\" or r.type == \"NEG_AUTHOR_FROM\":\n",
    "            res[key_subj].append((map_type_to_tag(entities[r.obj].text), entities[r.obj].text))\n",
    "    return res\n",
    "\n",
    "def flatten_data(_data, ids):\n",
    "    data = _data.copy()\n",
    "    res = data\n",
    "    delete_ids = []\n",
    "    for par in ids:\n",
    "            res[par[0]] = np.mean(data[par[0]:par[1]], axis=0)\n",
    "            res.T[par[0]] = np.sum(data.T[par[0]:par[1]], axis=0)\n",
    "            delete_ids = delete_ids + list(range(par[0]+1, par[1]))  \n",
    "    \n",
    "    res = np.delete(res, delete_ids, axis=1)\n",
    "    return np.delete(res, delete_ids, axis=0)\n",
    "\n",
    "def flatten_attention_with_ids(attention, tokens, ids, join_symbol = \" \"):\n",
    "    res = \"\"\n",
    "    for i in range(ids[0], ids[1]):\n",
    "        if i == ids[0]:\n",
    "            res += tokens[i]\n",
    "        else:\n",
    "            res += join_symbol + tokens[i]\n",
    "\n",
    "    \n",
    "    flatten_tokens = np.concatenate([tokens[:ids[0]], np.array([res]), tokens[ids[1]:]])    \n",
    "            \n",
    "    flatten_attention = []\n",
    "    for i in range(len(attention)):\n",
    "        one_layer_at = []\n",
    "        for j in range(len(attention[i][0])):\n",
    "            new_data = flatten_data(attention[i][0][j].detach().numpy(), np.array([ids]))\n",
    "            one_layer_at.append(new_data)\n",
    "\n",
    "        flatten_attention.append(torch.from_numpy(np.array([one_layer_at])))\n",
    "    flatten_attention = tuple(flatten_attention)\n",
    "        \n",
    "    return flatten_attention, flatten_tokens;\n",
    "\n",
    "def concat_attention(att, tokens):\n",
    "    attention = tuple(list(att))\n",
    "    conc_tokens = []\n",
    "    ids = []\n",
    "    i = 0\n",
    "    n = len(tokens)-1\n",
    "    \n",
    "    while i <= n:\n",
    "        if i < n and len(tokens[i+1]) > 2 and tokens[i+1][:2] == \"##\":\n",
    "            k = 0\n",
    "            res = tokens[i]\n",
    "            while i+k+1 <= n and tokens[i+k+1][:2] == \"##\":\n",
    "                res = res + tokens[i+1+k][2:]\n",
    "                k = k+1\n",
    "\n",
    "            ids.append((i, i+k+1))\n",
    "            conc_tokens.append(res)\n",
    "            i = i+k+1\n",
    "            \n",
    "        else:\n",
    "            if len(tokens[i]) < 3 or tokens[i][:2] != \"##\":\n",
    "                conc_tokens.append(tokens[i])\n",
    "            else:\n",
    "                continue\n",
    "            i = i+1\n",
    "    \n",
    "    idx_1 = np.where(np.array(conc_tokens) == \",\")[0]\n",
    "    idx_2 = np.where(np.array(conc_tokens) == \".\")[0]\n",
    "    idx_3 = np.where(np.array(conc_tokens) == \"«\")[0]\n",
    "    idx_4 = np.where(np.array(conc_tokens) == \"»\")[0]\n",
    "    idx_5 = np.where(np.array(conc_tokens) == \"—\")[0]\n",
    "    idx_6 = np.where(np.array(conc_tokens) == \")\")[0]\n",
    "    idx_7 = np.where(np.array(conc_tokens) == \"(\")[0]\n",
    "    idx_8 = np.where(np.array(conc_tokens) == '\"')[0]\n",
    "    idx_9 = np.where(np.array(conc_tokens) == \"'\")[0]\n",
    "    idx_10 = np.where(np.array(conc_tokens) == \":\")[0]\n",
    "    \n",
    "    idxs = np.concatenate([idx_1, idx_2, idx_3, idx_4, idx_5, idx_6, idx_7, idx_8, idx_9, idx_10])\n",
    "        \n",
    "    conc_attention = []\n",
    "    for i in range(len(attention)):\n",
    "        one_layer_at = []\n",
    "        for j in range(len(attention[i][0])):\n",
    "            new_data = flatten_data(attention[i][0][j].detach().numpy(), np.array(ids))\n",
    "            res_at = np.delete(new_data, idxs, axis=1)\n",
    "            one_layer_at.append(np.delete(res_at, idxs, axis=0))\n",
    "\n",
    "        conc_attention.append(torch.from_numpy(np.array([one_layer_at])))\n",
    "    conc_attention = tuple(conc_attention)   \n",
    "    \n",
    "    \n",
    "    return conc_attention, np.delete(conc_tokens, idxs)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \n",
    "    f1_macro = f1_score(\n",
    "      y_true=y_true,\n",
    "      y_pred=y_pred,\n",
    "      average=\"macro\",\n",
    "      zero_division=0,\n",
    "    )\n",
    "    \n",
    "    accuracy = accuracy_score(\n",
    "      y_true=y_true,\n",
    "      y_pred=y_pred,\n",
    "    )\n",
    "    \n",
    "    return f1_macro, accuracy\n",
    "\n",
    "def mean_by_head(attention):\n",
    "    res = []\n",
    "    for item in attention:\n",
    "        a = item.clone()\n",
    "        one_layer_at = []\n",
    "        mean_at = np.mean(a[0].detach().numpy(), axis=0)\n",
    "        res.append(mean_at)\n",
    "    \n",
    "    return np.array(res)            \n",
    "\n",
    "def mean_by_layer(attention):\n",
    "    res = []\n",
    "    for item in attention:\n",
    "        a = item.clone()\n",
    "        res.append(a[0].detach().numpy())\n",
    "    \n",
    "    return np.mean(np.array(res), axis=0)\n",
    "\n",
    "def mean_by_all(attention, st=0, en=12):\n",
    "    res = []\n",
    "    for item in attention[st:en]:\n",
    "        a = item.clone()\n",
    "        res.append(a[0].detach().numpy())\n",
    "    \n",
    "    return np.mean(np.mean(np.array(res), axis=0), axis=0)\n",
    "\n",
    "def map_sign_for_tag(sign):\n",
    "    if sign == '+':\n",
    "        return 1\n",
    "    else: \n",
    "        return -1\n",
    "\n",
    "def result_by_multiple_attentions(mean_attentions, tokens, entity_ids, ids, tags): \n",
    "    if len(ids) == 0:\n",
    "        print(0)\n",
    "    else:    \n",
    "        for att in mean_attentions:\n",
    "            max_id = np.argmax(att[entity_id][ids])\n",
    "            max_attention_item = tokens[ids[max_id]]\n",
    "            print(att[entity_id][ids], max_attention_item, tags[max_id])\n",
    "        \n",
    "def result_by_total_attention(mean_attention, tokens, _entity_ids, _ids, tags, log = False, _aspect_ids=[]): \n",
    "    result = 0\n",
    "    ids = _ids\n",
    "    entity_ids = _aspect_ids + _entity_ids\n",
    "    if len(ids) != 0:\n",
    "        if len(entity_ids) + len(aspect_ids) == 1:\n",
    "            result = 0\n",
    "            max_id = np.argmax(mean_attention[entity_ids[0]][ids])\n",
    "            max_attention_item = tokens[ids[max_id]]\n",
    "            result = map_sign_for_tag(tags[max_id])\n",
    "\n",
    "        else:\n",
    "            while len(ids) != 0:\n",
    "                if log:\n",
    "                    print(entity_ids)\n",
    "                    print(ids)\n",
    "                    print(tags)\n",
    "                max_id = np.argmax(mean_attention[entity_ids[-1]][ids])\n",
    "                res_id = ids[max_id]\n",
    "                if log:\n",
    "                    print(mean_attention[entity_ids[-1]][ids])\n",
    "                if log:\n",
    "                    print(tokens[entity_ids[-1]], mean_attention[entity_ids[-1]][res_id], tokens[res_id])\n",
    "\n",
    "                entity_atts = mean_attention[entity_ids]\n",
    "                entity_results = entity_atts[np.arange(len(entity_atts)), res_id]\n",
    "                _id = np.argmax(entity_results)\n",
    "\n",
    "                if log:\n",
    "                    print(tokens[entity_ids[_id]], mean_attention[entity_ids[_id]][res_id], tokens[res_id], '\\n')\n",
    "\n",
    "                if np.max(entity_results) == entity_results[-1]:\n",
    "                    result = map_sign_for_tag(tags[max_id])\n",
    "                    break\n",
    "                else:\n",
    "                    if entity_ids[_id] in _aspect_ids:\n",
    "                        from_ent_to_aspect_id = mean_attention[entity_ids[-1]][entity_ids[_id]]\n",
    "                        from_ent_to_tone_id = mean_attention[entity_ids[-1]][res_id]\n",
    "                        \n",
    "                        if log:\n",
    "                            print(tokens[entity_ids[-1]], from_ent_to_aspect_id, tokens[entity_ids[_id]])\n",
    "                            print(tokens[entity_ids[-1]], from_ent_to_tone_id, tokens[res_id], '\\n')\n",
    "                        \n",
    "                        if from_ent_to_aspect_id > from_ent_to_tone_id:\n",
    "                            result = map_sign_for_tag(tags[max_id])\n",
    "                            break\n",
    "                    \n",
    "                    entity_ids.pop(_id)\n",
    "                    ids.pop(max_id)\n",
    "                    tags.pop(max_id)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def ids_of_ents(tokens, ent):\n",
    "    ent_parts = nltk.word_tokenize(ent)\n",
    "    res = []\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == ent:\n",
    "            return [i, i+1]\n",
    "        elif tokens[i] in ent_parts:\n",
    "            res.append(i)\n",
    "            k = i + 1\n",
    "            while k < len(tokens) and tokens[k] in ent_parts:\n",
    "                k += 1\n",
    "            res.append(k)\n",
    "            break\n",
    "    return res\n",
    "\n",
    "    \n",
    "def aspect_is_valid(aspect, entities, sentiments):\n",
    "    for entity in entities:\n",
    "        if entity in aspect or aspect in entity:\n",
    "            return False\n",
    "        \n",
    "    for sent in sentiments:\n",
    "        if sent in aspect or aspect in sent:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "    \n",
    "    \n",
    "def parse_smta_data(start = 0, n = 100, multiple_ne = False, nlp = None, ids_to_use=None, log=True):\n",
    "    \n",
    "    result = []\n",
    "    true_labels = []\n",
    "    checked_ids = []\n",
    "    labeled_data, labels = read_data()\n",
    "    \n",
    "    for i in range(start, n):\n",
    "        try:\n",
    "            if ids_to_use is None or i in ids_to_use:\n",
    "                checked_ids.append(i)\n",
    "                data_item, label = labeled_data[i], labels[i] \n",
    "                true_labels.append(label)\n",
    "                file = open('train_data/output_' + str(i) + '.txt', 'r')\n",
    "                data_str = file.read()\n",
    "                data = json.loads(data_str)\n",
    "                _attention, _tokens = compute_tokens_and_attention(data['text'])\n",
    "                attention, tokens = concat_attention(_attention, _tokens)\n",
    "                ids_of_d = ids_of_dashes(tokens.tolist())\n",
    "\n",
    "                while not (ids_of_d is None):\n",
    "                    attention, tokens = flatten_attention_with_ids(attention, tokens, ids_of_d, join_symbol='')\n",
    "                    ids_of_d = ids_of_dashes(tokens.tolist())\n",
    "                sentiments = []\n",
    "                smta_items = []\n",
    "       \n",
    "                for item in data['index']:\n",
    "                    if item['type'] == 'SMTA':\n",
    "                        if data_item[1] in tokens[item['pos'] - 1 : item['pos'] + item['len'] - 1]:\n",
    "                            continue\n",
    "                        else:    \n",
    "                            smta_items.append(item)\n",
    "                            if log:\n",
    "                                print('SMTA', tokens[item['pos'] - 1 : item['pos'] + item['len'] - 1], item['name'])\n",
    "                            sentiments.append(tokens[item['pos'] - 1 : item['pos'] + item['len'] - 1])\n",
    "                    elif item['type'] == 'SMTAW':\n",
    "                        if log:\n",
    "                            print('SMTAW', tokens[item['pos'] - 1 : item['pos'] + item['len'] - 1])\n",
    "\n",
    "\n",
    "                entity_parts = nltk.word_tokenize(data_item[1])\n",
    "                entity_ids = sublist(entity_parts, tokens.tolist())\n",
    "                attention, tokens = flatten_attention_with_ids(attention, tokens, entity_ids)\n",
    "\n",
    "                for item in sentiments:\n",
    "                    ids = sublist(item.tolist(), tokens.tolist())\n",
    "                    if not (ids is None):\n",
    "                        attention, tokens = flatten_attention_with_ids(attention, tokens, ids)                \n",
    "                        \n",
    "                result_entities = []\n",
    "                if multiple_ne:\n",
    "                    found_entities = []\n",
    "                    for poses in entity_extraction([data['text']])[2][0]:\n",
    "                        found_entities.append(data['text'][poses[0]:poses[1]])\n",
    "                    for ent in found_entities:\n",
    "                        if ent != data_item[1]:\n",
    "                            ent_ids = ids_of_ents(tokens, ent)\n",
    "                            if len(ent_ids) > 0:\n",
    "                                if ent_ids[0] + 1 < ent_ids[1]:\n",
    "                                    attention, tokens = flatten_attention_with_ids(attention, tokens, ent_ids)\n",
    "                                \n",
    "                                result_entities.append(tokens[ent_ids[0]])    \n",
    "                                    \n",
    "                if log:\n",
    "                    print('\\n', tokens, '\\n')                \n",
    "                \n",
    "                new_ids = []\n",
    "                new_ids_no_none = []\n",
    "                for item in sentiments:\n",
    "                    text = ' '.join(item.tolist())\n",
    "                    new_id = sublist([text], tokens.tolist())\n",
    "                    if not (new_id is None):\n",
    "                        new_ids.append(new_id[0])\n",
    "                        new_ids_no_none.append(new_id[0])\n",
    "                    else:\n",
    "                        new_ids.append(None)\n",
    "    \n",
    "                result_entity_ids = []\n",
    "                for ent in result_entities:\n",
    "                    ne_id = sublist([ent], tokens.tolist())\n",
    "                    if not (ne_id is None):\n",
    "                        result_entity_ids.append(ne_id[0]) \n",
    "                \n",
    "                result_entity_ids.append(sublist([data_item[1]], tokens.tolist())[0])\n",
    "\n",
    "                aspect_ids = []\n",
    "                if multiple_ne:\n",
    "                    doc = nlp(data['text'])\n",
    "                    for token in doc:\n",
    "                        if (token.pos_ == 'NOUN' or token.pos_ == 'PROPN') and aspect_is_valid(\n",
    "                            aspect = token.text, \n",
    "                            entities = tokens[result_entity_ids], \n",
    "                            sentiments = tokens[new_ids_no_none],\n",
    "                        ):\n",
    "                            ne_id = sublist([token.text], tokens.tolist())\n",
    "                            if not (ne_id is None):\n",
    "                                aspect_ids.append(ne_id[0]) \n",
    "                \n",
    "                if multiple_ne and log:\n",
    "                    print('entities:', tokens[result_entity_ids], '\\n')\n",
    "                    print('aspects:', tokens[aspect_ids], '\\n')\n",
    "                    print('sentiments:', tokens[new_ids_no_none], '\\n')\n",
    "\n",
    "\n",
    "                ids = []\n",
    "                tags = []\n",
    "                for i in range(len(new_ids)):\n",
    "                    if not (new_ids[i]  is None):\n",
    "                        ids.append(new_ids[i])\n",
    "                        tags.append(smta_items[i]['name'])\n",
    "\n",
    "                result.append((attention, tokens, result_entity_ids, ids, tags, aspect_ids))\n",
    "        except:\n",
    "            true_labels.pop()\n",
    "            checked_ids.pop()\n",
    "            continue\n",
    "\n",
    "    return result, true_labels, checked_ids\n",
    "\n",
    "def count_score(attention, tokens, entity_ids, ids, tags, aspect_ids, log):\n",
    "    mean_attention = mean_by_all(attention)\n",
    "    return result_by_total_attention(mean_attention, tokens, entity_ids, ids, tags, log, aspect_ids)\n",
    "\n",
    "def show_attention(\n",
    "    attention, \n",
    "    from_tokens, \n",
    "    to_tokens, \n",
    "    width=12, \n",
    "    height=0.5, \n",
    "    labelsize=8, \n",
    "    bottom_rot=0, \n",
    "    annot=True,\n",
    "    fontsize=8,\n",
    "    xlabel=None,\n",
    "    ylabel=None\n",
    "):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    ax = sns.heatmap(\n",
    "        attention, \n",
    "        cmap=\"rocket_r\", \n",
    "        xticklabels=to_tokens, \n",
    "        yticklabels=from_tokens, \n",
    "        annot=annot, \n",
    "        annot_kws={\"fontsize\":fontsize},\n",
    "        vmax=0.8\n",
    "    )\n",
    "    ax.tick_params(labelsize=labelsize)\n",
    "    \n",
    "    if not (xlabel is None):\n",
    "        ax.set_xlabel(xlabel, fontsize=18, labelpad=20)\n",
    "    if not (ylabel is None):\n",
    "        ax.set_ylabel(ylabel, fontsize=25)\n",
    "        \n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=bottom_rot)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
