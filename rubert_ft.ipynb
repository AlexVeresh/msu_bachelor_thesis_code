{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef287d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoTokenizer, AutoModel, PreTrainedTokenizer, AutoModelForSequenceClassification\n",
    "from bertviz import head_view, model_view\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers.trainer_callback import dataclass\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int) -> None:\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043047aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label2idx(label_set: List[str]) -> Dict[str, int]:\n",
    "\n",
    "    label2idx: Dict[str, int] = {}\n",
    "\n",
    "    it = 0\n",
    "    for label in label_set:\n",
    "        label2idx[label] = it\n",
    "        it += 1\n",
    "\n",
    "    return label2idx\n",
    "\n",
    "label2idx = get_label2idx(['-1', '0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0865e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset() -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \n",
    "    df = pd.read_csv(\"sample_data/train_data.csv\", sep='\\t') #train_data\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "        \n",
    "    for _, item in df.iterrows():\n",
    "        origin = item['sentence']\n",
    "        data.append(origin)\n",
    "        labels.append(item['label'])\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "class TransformersDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[List[str]],\n",
    "        labels: List[str],\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.labels = self.process_labels(labels, label2idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> Tuple[List[str], int]:\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels: List[str],\n",
    "        label2idx: Dict[str, int],\n",
    "    ) -> List[int]:\n",
    "       \n",
    "        l_indices = [] \n",
    "        for label in labels:\n",
    "            # print(label2idx)\n",
    "            # print(label)\n",
    "            l_indices.append(label2idx[str(label)])\n",
    "            \n",
    "        return l_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38661a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersCollator:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        tokenizer_kwargs: Dict[str, Any],\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_kwargs = tokenizer_kwargs\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: List[Tuple[List[str], int]],\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        data, labels = zip(*batch)\n",
    "\n",
    "        data = self.tokenizer(list(data), **self.tokenizer_kwargs)\n",
    "\n",
    "        data.pop(\"offset_mapping\")\n",
    "\n",
    "        return data, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = parse_dataset()\n",
    "dataset = TransformersDataset(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa949ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = random_split(dataset, [5637, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3895cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_kwargs = {\n",
    "    #\"is_split_into_words\":    True,\n",
    "    \"return_offsets_mapping\": True,\n",
    "    \"padding\":                True,\n",
    "    \"truncation\":             True,\n",
    "    \"max_length\":             512,\n",
    "    \"return_tensors\":         \"pt\",\n",
    "    \"add_special_tokens\":    False\n",
    "}\n",
    "collator = TransformersCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "tokens, labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e391a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels = 3, \n",
    "    output_attentions=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ff5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "\n",
    "  f1_macro = f1_score(\n",
    "      y_true=y_true.flatten().cpu(),\n",
    "      y_pred=torch.argmax(y_pred, axis=1).flatten().cpu(),\n",
    "      average=\"macro\",\n",
    "      zero_division=0,\n",
    "    )\n",
    "  \n",
    "  accuracy = accuracy_score(\n",
    "      y_true=y_true.flatten().cpu(),\n",
    "      y_pred=torch.argmax(y_pred, axis=1).flatten().cpu(),\n",
    "  )\n",
    "  return f1_macro, accuracy\n",
    "\n",
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_f1_metrics_list = []\n",
    "    batch_ac_metrics_list = []\n",
    "\n",
    "    for i, (tokens, labels) in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        desc=\"loop over train batches\",\n",
    "    ):\n",
    "\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**tokens)\n",
    "        loss = criterion(outputs[\"logits\"], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs_inference = model(**tokens)[\"logits\"]\n",
    "            model.train()\n",
    "\n",
    "        f1_metric, ac_metric = compute_metrics(\n",
    "            y_true=labels,\n",
    "            y_pred=outputs_inference,\n",
    "        )\n",
    "       \n",
    "        batch_f1_metrics_list.append(f1_metric)\n",
    "        batch_ac_metrics_list.append(ac_metric)\n",
    "\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Train loss: {avg_loss}\\n\")\n",
    "\n",
    "    f1_metric_per_batch = np.mean(batch_f1_metrics_list)\n",
    "    ac_metric_per_batch = np.mean(batch_ac_metrics_list)\n",
    "    print(f\"Train F1-macro: {f1_metric_per_batch}\\n\")\n",
    "    print(f\"Train Accuracy: {ac_metric_per_batch}\\n\")\n",
    "\n",
    "def evaluate_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "  \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_f1_metrics_list = []\n",
    "    batch_ac_metrics_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (tokens, labels) in tqdm(\n",
    "            enumerate(dataloader),\n",
    "            total=len(dataloader),\n",
    "            desc=\"loop over test batches\",\n",
    "        ):\n",
    "\n",
    "            tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(**tokens)[\"logits\"]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            f1_metric, ac_metric = compute_metrics(\n",
    "                y_true=labels,\n",
    "                y_pred=outputs,\n",
    "            )\n",
    "          \n",
    "            batch_f1_metrics_list.append(f1_metric)\n",
    "            batch_ac_metrics_list.append(ac_metric)\n",
    "\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"Test loss:  {avg_loss}\\n\")\n",
    "\n",
    "        f1_metric_per_batch = np.mean(batch_f1_metrics_list)\n",
    "        ac_metric_per_batch = np.mean(batch_ac_metrics_list)\n",
    "        print(f\"Test F1-macro: {f1_metric_per_batch}\\n\")\n",
    "        print(f\"Test Accuracy: {ac_metric_per_batch}\\n\")\n",
    "\n",
    "def train_with_val(\n",
    "    n_epochs: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "\n",
    "        train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        evaluate_epoch(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        ) \n",
    "\n",
    "def train(\n",
    "    n_epochs: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "\n",
    "        train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_val(5, model, train_dataloader, valid_dataloader, optimizer, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
